import feedparser
import json
import datetime
import os
import hashlib
import re

# 1. è¨­å®šæª”æ¡ˆè·¯å¾‘èˆ‡ RSS ä¾†æº
DATA_FILE = 'bear_data.json'
RSS_URL = 'https://news.google.com/rss/search?q=ç†Š+å‡ºæ²¡+when:1d&hl=ja&gl=JP&ceid=JP:ja'

# 2. ç°¡æ˜“åº§æ¨™å°ç…§è¡¨ (å¯¦éš›å°ˆæ¡ˆå»ºè­°æ¥ Google Maps API æˆ– Nominatim)
PREFECTURE_COORDS = {
    "åŒ—æµ·é“": {"lat": 43.066666, "lng": 141.35},
    "æœ­å¹Œ":   {"lat": 43.061771, "lng": 141.354506},
    "é’æ£®":   {"lat": 40.822222, "lng": 140.7475},
    "å²©æ‰‹":   {"lat": 39.703611, "lng": 141.156389},
    "å®®åŸ":   {"lat": 38.268222, "lng": 140.869417},
    "ç§‹ç”°":   {"lat": 39.716667, "lng": 140.1025},
    "å±±å½¢":   {"lat": 38.255556, "lng": 140.339722},
    "ç¦å³¶":   {"lat": 37.760833, "lng": 140.474722},
    "é•·é‡":   {"lat": 36.648056, "lng": 138.194722},
    "æ–°æ½Ÿ":   {"lat": 37.902222, "lng": 139.023611},
    "å¯Œå±±":   {"lat": 36.695278, "lng": 137.211389},
    "çŸ³å·":   {"lat": 36.594444, "lng": 136.625556},
    "ç¦äº•":   {"lat": 36.064722, "lng": 136.219444},
    "ç¾¤é¦¬":   {"lat": 36.390556, "lng": 139.060278},
    "æ ƒæœ¨":   {"lat": 36.565833, "lng": 139.883611}
}

def load_data():
    if not os.path.exists(DATA_FILE):
        return []
    with open(DATA_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)

def save_data(data):
    # æŒ‰æ—¥æœŸå€’åºæ’åˆ—
    data.sort(key=lambda x: x['date'], reverse=True)
    with open(DATA_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

def get_coordinates(text):
    """
    å¾æ¨™é¡Œæˆ–æè¿°ä¸­æå–åœ°åä¸¦è¿”å›åº§æ¨™ã€‚
    é€™æ˜¯ä¸€å€‹ç°¡åŒ–ç‰ˆï¼Œå„ªå…ˆåŒ¹é…å…·é«”åŸå¸‚ï¼Œå†åŒ¹é…ç¸£ã€‚
    """
    for place, coords in PREFECTURE_COORDS.items():
        if place in text:
            # ç‚ºäº†é¿å…æ‰€æœ‰é»éƒ½é‡ç–Šï¼Œé€™è£¡å¯ä»¥åŠ å…¥å¾®å°çš„éš¨æ©Ÿåç§» (jitter)
            # ä½†ç‚ºäº†æ¼”ç¤ºæ¸…æ™°ï¼Œå…ˆç›´æ¥è¿”å›ä¸­å¿ƒé»
            return coords
    return None # æ‰¾ä¸åˆ°åœ°é»

def update_feed():
    print(f"ğŸ”„ é–‹å§‹æŠ“å–æ–°è: {datetime.datetime.now()}")
    
    current_data = load_data()
    existing_links = {item['link'] for item in current_data}
    
    feed = feedparser.parse(RSS_URL)
    new_entries = []

    for entry in feed.entries:
        # æª¢æŸ¥æ˜¯å¦å·²ç¶“å­˜åœ¨
        if entry.link in existing_links:
            continue

        title = entry.title
        published = entry.published_parsed
        # å°‡ struct_time è½‰ç‚ºå­—ä¸²
        pub_date = datetime.datetime(*published[:6]).strftime("%Y-%m-%d %H:%M:%S")
        
        # ç°¡å–®éæ¿¾ï¼šåªæŠ“å–æ¨™é¡Œå«æœ‰ã€Œç†Šã€æˆ–ã€Œã‚¯ãƒã€çš„æ–°è
        if "ç†Š" not in title and "ã‚¯ãƒ" not in title:
            continue

        # å˜—è©¦è§£æåœ°é»
        coords = get_coordinates(title)
        
        # å¦‚æœæ‰¾ä¸åˆ°åœ°é»ï¼Œé è¨­ä¸åŠ å…¥ï¼Œæˆ–è€…å¯ä»¥è¨­ç‚ºæ—¥æœ¬ä¸­å¿ƒé»ä¸¦æ¨™è¨˜ç‚ºã€Œåœ°é»æœªè©³ã€
        if not coords:
            continue 

        # å»ºç«‹æ–°æ•¸æ“šç‰©ä»¶
        new_item = {
            "id": hashlib.md5(entry.link.encode()).hexdigest(),
            "title": title,
            "location": "æ–°èå ±å°åœ°é»", # é€™è£¡å¯ä»¥æ›´é€²éšç”¨ NLP æå–
            "lat": coords['lat'],
            "lng": coords['lng'],
            "date": pub_date,
            "link": entry.link,
            "source": entry.source.title if 'source' in entry else "Google News"
        }
        
        new_entries.append(new_item)
        print(f"âœ… ç™¼ç¾æ–°ç›®æ“Š: {title} ({pub_date})")

    if new_entries:
        current_data.extend(new_entries)
        save_data(current_data)
        print(f"ğŸ’¾ å·²æ›´æ–° {len(new_entries)} ç­†è³‡æ–™ã€‚")
    else:
        print("ğŸ’¤ æ²’æœ‰ç™¼ç¾æ–°è³‡æ–™ã€‚")

if __name__ == "__main__":
    update_feed()
